<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="第四周paper总结"><meta name="keywords" content="Named entity recognition,paper,Adaptation,Cross-domain,Zero-Resource"><meta name="author" content="Ian"><meta name="copyright" content="Ian"><title>第四周paper总结 | 小僧有点二</title><link rel="shortcut icon" href="https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?23e67fb29291e5dc0f0f7c9f882ac0ea";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"8XJAI86SDA","apiKey":"f1fda4bfbc8839aa09272440cf9215d1","indexName":"ian-peace","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#the-first-paper-adaptation-layers"><span class="toc-number">1.</span> <span class="toc-text"> The first paper, Adaptation Layers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#缺点"><span class="toc-number">1.1.</span> <span class="toc-text"> 缺点：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#论文方法"><span class="toc-number">1.2.</span> <span class="toc-text"> 论文方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实验"><span class="toc-number">1.3.</span> <span class="toc-text"> 实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#结论"><span class="toc-number">1.4.</span> <span class="toc-text"> 结论</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-second-paper-zero-resource-for-ner"><span class="toc-number">2.</span> <span class="toc-text"> The second paper, Zero-Resource for NER</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#两个创新点"><span class="toc-number">2.1.</span> <span class="toc-text"> 两个创新点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#具体过程"><span class="toc-number">2.2.</span> <span class="toc-text"> 具体过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实验-2"><span class="toc-number">2.3.</span> <span class="toc-text"> 实验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reference"><span class="toc-number">3.</span> <span class="toc-text"> Reference</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/xiaoseng.png"></div><div class="author-info__name text-center">Ian</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">10</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">27</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">2</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/top-img-75.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">小僧有点二</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">第四周paper总结</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-02</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Machine-Learning/">Machine Learning</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.8k</span><span class="post-meta__separator">|</span><span>Reading time: 5 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>两篇论文：</p>
<ul>
<li>Neural Adaptation Layers for Cross-domain Named Entity Recognition，在基础的迁移学习模型上添加了自适应层</li>
<li>Zero-Resource Cross-Domain Named Entity Recognition，如题，提出了只使用source领域的数据进行训练的跨领域实体识别模型</li>
</ul>
<p>博客地址：<a href="https://ian-peace.github.io">https://ian-peace.github.io</a></p>
<a id="more"></a>
<h2 id="the-first-paper-adaptation-layers"><a class="markdownIt-Anchor" href="#the-first-paper-adaptation-layers"></a> The first paper, Adaptation Layers</h2>
<p><em>《Neural Adaptation Layers for Cross-domain Named Entity Recognition》</em></p>
<p>​		这篇论文是18年的，比较旧，看它主要是因为它添加了自适应层，这个概念在其他论文里提到了很多次。文章提到当时NER任务的主要迁移学习模型包括INIT和MULT两种。</p>
<p><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/img/202004/0402-1.png" alt="Init model and Mult model" /></p>
<ul>
<li>INIT就是使用Target数据在通过Source数据训练的模型上进行微调，且在两个领域共享word embedding。</li>
<li>MULT就是多任务学习，它同时使用两个领域的数据训练两个模型，除CRF层外其他层的参数都共享。</li>
</ul>
<h3 id="缺点"><a class="markdownIt-Anchor" href="#缺点"></a> 缺点：</h3>
<ol>
<li>相同词在不同领域表达的意义可能不同，即存在domain shift（域位移），所以共享word embedding是不合适的。</li>
<li>这两种模型都只是针对最后的CRF层进行再训练，指望通过CRF就能较好地捕捉不同领域的语义信息，作者认为不够现实。</li>
<li>在不同特定领域进行迁移，就需要花费大量的时间重新在特定语料上对word embedding进行训练，这不现实。</li>
</ol>
<p><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/img/202004/0402-2.png" alt="Method" /></p>
<h3 id="论文方法"><a class="markdownIt-Anchor" href="#论文方法"></a> 论文方法</h3>
<p>在INIT模型基础上，增加了三个自适应层，且不同层有不同的learning rate。</p>
<ul>
<li>
<p>Word Adaptation Layer</p>
<p>初衷是为了既可以通过利用target领域和source领域的不同数据去学习特定领域下的不同word embedding，同时又可以不对巨大的source domain corpora进行再训练，因此一个很自然的想法就是将target embedding与source embedding关联起来，即完成两个embedding空间的相关映射。</p>
<p>具体做法：针对在source和target两个领域数据中出现频率较多的词建立词典，见公式，词典以二元组的形式构建，其中$$f(w)$$ 是单词出现得频率，$$\phi$$是阈值。之后用词典中的两项分别构建两个word embedding VS和VT，目标即为学习到最优的变换矩阵Z，使得其将target vector representation转换后与source vector representation的差异最小。其中c是confidence系数，由这三个公式去求，前两个是为了归一化f，第三个公式是根据<strong>Dice系数</strong>得到的，它是一种集合相似度的度量函数，通常用于计算两个样本的相似度。</p>
</li>
<li>
<p>Sentence Adaptation Layer</p>
<p>Word自适应层仅仅是在单词层面上关联两个领域，而且是上下文无关的。为了在embedding上添加上下文信息，在word自适应层之后增加了一层BiLSTM作为sentence自适应层，这一层对每个target领域的实例的embedding投影进行预编码，即对word embedding添加上下文信息进一步转换。同时该层还可以根据上下文调整OOV的表示。</p>
</li>
<li>
<p>Output Adaptation Layer</p>
<p>由于source领域和target领域模型得到的预测标签分布可能不一致，例如，“Taylor released her new songs”这句话中的第一个单词，source领域被标记为人名，target领域被标记为音乐家。所以对实体进行重新分类和重新识别就是必要的，故在BiLSTM结构之后添加了一层BiLSTM作为Output自适应层，同时该层也避免了直接在base模型上更新参数导致从source领域迁移得到的知识被损失掉的问题。</p>
</li>
</ul>
<h3 id="实验"><a class="markdownIt-Anchor" href="#实验"></a> 实验</h3>
<p>消融实验，证明了三种自适应层的有效性。</p>
<h3 id="结论"><a class="markdownIt-Anchor" href="#结论"></a> 结论</h3>
<ul>
<li>比较有创新点的工作是word自适应层的提出。</li>
<li>在跨领域NER任务上提出了一种新颖、轻量的迁移学习方法，且该方法还可以应用于其他跨领域任务中。</li>
</ul>
<h2 id="the-second-paper-zero-resource-for-ner"><a class="markdownIt-Anchor" href="#the-second-paper-zero-resource-for-ner"></a> The second paper, Zero-Resource for NER</h2>
<p><em>《Zero-Resource Cross-Domain Named Entity Recognition》</em></p>
<p>现有的跨领域实体识别模型依赖于target领域大量未标记或已标记的语料，但是低资源的目标领域收集数据很困难。这篇论文提出了一个不使用任何外部资源的跨领域NER模型，即只使用source领域的数据进行训练。具体实现方法也是论文的两个创新点。</p>
<p><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/img/202004/0402-3.png" alt="Model" /></p>
<h3 id="两个创新点"><a class="markdownIt-Anchor" href="#两个创新点"></a> 两个创新点</h3>
<ol>
<li>
<p>MTL: multi-task learning</p>
<p>由于跨领域命名实体之间的巨大差异，导致无监督跨领域NER模型无法识别命名实体，那论文就提出了一个新的目标函数用于预测每个单词是否为实体，就是改成二分类任务，以此来学习命名实体的通用表示，从而增强领域自适应性的鲁棒性。在图中表示为任务1，任务2才是对实体类别的具体判断。</p>
</li>
<li>
<p>MoEE: Mixture of Entity Experts</p>
<p>我们可以注意到，在许多情况下，不同的实体类别可能具有相似或相同的上下文。例如：“Arafat subsequently canceled a meeting between Israeli and PLO officials,”这个句子中的第一个单词是人名，但通过上下文来看这里也可以被替换为一个机构名。这说明了不同实体类别之间的混乱性，这样的实体判断在零资源的目标领域会更加困难，也会导致模型容易在source领域过拟合从而损失在target领域的生成能力。所以作者提出了MoEE结构，每个单词实体类型的判断由所有实体专家一起加权判断。在图中表示为任务2.</p>
<p>具体来讲，就是每一个实体类别都被看作是一个由线性层构成的实体专家，包括非实体类别。这里的The expert gate 由一个线性层和一个softmax层组成，softmax层生成实体专家的confidence分布。最后，The meta-expert feature基于这个confidence分布，整合所有专家的特征。</p>
</li>
</ol>
<h3 id="具体过程"><a class="markdownIt-Anchor" href="#具体过程"></a> 具体过程</h3>
<p>​		整个过程对应的公式如下：</p>
<p><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/img/202004/0402-4.png" alt="Formula" /></p>
<p>​		两个任务以及The expert gate对应的三个损失函数分别为右侧的三个式子。</p>
<h3 id="实验-2"><a class="markdownIt-Anchor" href="#实验-2"></a> 实验</h3>
<ul>
<li>
<p>实验数据：CoNLL-2003 English NER data作为source data，SciTech News作为target data。Embedding使用FastText embedding和BERT，以解决OOV的问题。</p>
</li>
<li>
<p>实验结果：</p>
<p><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/img/202004/0402-5.png" alt="Result" /></p>
<ol>
<li>加上提出的两个任务，比基础的BiLSTM-CRF有明显的提高，证明了有效性。</li>
<li>固定的FastText embedding结果比BERT好，作者推测是由于Bert利用字词embedding，损失了一部分词级别信息。</li>
<li>最好的结果接近前两周我们提到的论文《Cross-Domain NER using Cross-Domain Language Modeling》的结果，但该论文使用了大量的数据语料，包括source和target两个领域。</li>
<li>论文还对confidence分布作了可视化，颜色越深代表实体专家给这个词的权重越大。</li>
</ol>
</li>
</ul>
<h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2>
<p>[1] Neural Adaptation Layers for Cross-domain Named Entity Recognition.</p>
<p>[2] Zero-Resource Cross-Domain Named Entity Recognition.</p>
<p>[3] NER- 命名实体识别(Chinese NER 、Cross-domain NER). <a href="https://zhuanlan.zhihu.com/p/67458346" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/67458346</a></p>
<p>[4] 零资源跨领域命名实体识别. <a href="https://zhuanlan.zhihu.com/p/107747571" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/107747571</a>.</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Ian</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://ian-peace.github.io/2020/04/02/20200401paper/">http://ian-peace.github.io/2020/04/02/20200401paper/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Named-entity-recognition/">Named entity recognition</a><a class="post-meta__tags" href="/tags/paper/">paper</a><a class="post-meta__tags" href="/tags/Adaptation/">Adaptation</a><a class="post-meta__tags" href="/tags/Cross-domain/">Cross-domain</a><a class="post-meta__tags" href="/tags/Zero-Resource/">Zero-Resource</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/ian-peace.png"><div class="post-qr-code__desc">微信订阅号</div></div></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/04/09/20200409paper/"><i class="fa fa-chevron-left">  </i><span>第五周paper总结</span></a></div><div class="next-post pull-right"><a href="/2020/03/25/20200324paper/"><span>第三周paper总结</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '73becf39601f4aaa0a91',
  clientSecret: '6da05639806d66834500b149e950cd54c3d3737c',
  repo: 'Blog-Comment-Repo',
  owner: 'Ian-peace',
  admin: 'Ian-peace',
  id: md5(decodeURI(location.pathname)),
  language: 'en'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/top-img-75.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2020 By Ian</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><script src="/js/search/algolia.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>