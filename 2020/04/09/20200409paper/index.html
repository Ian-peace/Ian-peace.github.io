<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="第五周paper总结"><meta name="keywords" content="paper,Named Entity Recognition,multilingual,Bert,Meta-learning"><meta name="author" content="Ian"><meta name="copyright" content="Ian"><title>第五周paper总结 | 小僧有点二</title><link rel="shortcut icon" href="https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?23e67fb29291e5dc0f0f7c9f882ac0ea";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"8XJAI86SDA","apiKey":"f1fda4bfbc8839aa09272440cf9215d1","indexName":"ian-peace","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#the-first-paper-multilingual-bert"><span class="toc-number">1.</span> <span class="toc-text"> The first paper, Multilingual BERT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#实验"><span class="toc-number">1.1.</span> <span class="toc-text"> 实验：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-second-paper-meta-learning-for-ner"><span class="toc-number">2.</span> <span class="toc-text"> The second paper, Meta-Learning for NER</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#meta-learning"><span class="toc-number">2.1.</span> <span class="toc-text"> Meta-Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#meta-training-过程"><span class="toc-number">2.2.</span> <span class="toc-text"> Meta-training 过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#两个策略"><span class="toc-number">2.2.1.</span> <span class="toc-text"> 两个策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#adaptation-阶段即微调使用目标语言的test集"><span class="toc-number">2.3.</span> <span class="toc-text"> Adaptation 阶段，即微调，使用目标语言的test集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实验-2"><span class="toc-number">2.4.</span> <span class="toc-text"> 实验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reference"><span class="toc-number">3.</span> <span class="toc-text"> Reference</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/xiaoseng.png"></div><div class="author-info__name text-center">Ian</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">9</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">25</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">2</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/top-img-75.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">小僧有点二</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">第五周paper总结</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-04-09</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Machine-Learning/">Machine Learning</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.7k</span><span class="post-meta__separator">|</span><span>Reading time: 5 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>两篇论文：</p>
<ul>
<li>
<p>How multilingual is Multilingual BERT?  提出了多语言Bert。</p>
</li>
<li>
<p>Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with</p>
<p>Minimal Resources，它使用了多语言BERT模型，Meta-Learning算法做跨语言的NER任务。</p>
</li>
</ul>
<p>博客地址：<a href="https://ian-peace.github.io">https://ian-peace.github.io</a></p>
<a id="more"></a>
<h2 id="the-first-paper-multilingual-bert"><a class="markdownIt-Anchor" href="#the-first-paper-multilingual-bert"></a> The first paper, Multilingual BERT</h2>
<p><em>《How multilingual is Multilingual BERT?》</em></p>
<p>​		它使用的模型和原始的Bert模型一样，不同的是原始的Bert训练使用的是单一的英文数据以及由英文衍生的词汇表。而这篇论文在维基百科的104种语言页面上进行训练，并且共享一个词汇表。在模型训练期间，没有任何的标注，也没有任何翻译机制来对齐。模型上没有什么创新，但作者做了很多实验，证明了M-Bert有良好跨语言迁移能力。</p>
<h3 id="实验"><a class="markdownIt-Anchor" href="#实验"></a> 实验：</h3>
<ol>
<li>
<p>作者用了两个任务进行实验，NER和POS，实验方式<strong>就是用一种语言上对该任务进行微调，最后再在另一种语言上进行评估</strong>。结果见表1所示：</p>
<p><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/img/202004/0409-1.jpg" alt="Result" /></p>
<p>可以看到的是在相似语言上进行迁移学习效果会比较好，这也符合常理。</p>
</li>
<li>
<p><strong>但随之而来的想法就是</strong>，效果好是否是因为词汇重叠，就是finetune中出现的单词也出现在评估使用的语言中。<strong>所以作者为了探索这种跨语言的迁移有多大程度上时依赖于这种重叠，做了重叠实验</strong>，结果如右侧的图所示：下面是重叠的计算公式，就是训练集和验证集的交集除以并集。</p>
<p>实验包含M-Bert和英文Bert，可以看到英文BERT的性能表现非常依赖于词汇重叠，而多语言BERT则在大范围重叠率上表现得非常平缓，这证明多语言BERT在某种程度上拥有超过浅层词汇级别的深层次表征能力。</p>
</li>
<li>
<p><strong>作者还在语序上做了实验</strong>，结果见下面这两张表，其中SVO指主谓宾顺序例如英语，SOV指主宾谓顺序例如日语，AN是形容词名词顺序，NA是名词形容词顺序。可以看到语序一致的情况下性能会更好。也就是说，多语言Bert能够学习到一定的多语言特征，但是没有学习到这些语言的结构转换方式，以至于无法去适应带有不同语言顺序的目标语言。</p>
</li>
<li>
<p>作者还做了很多实验，但总的来说就是多语言的Bert具有很好的跨语言迁移能力。</p>
</li>
</ol>
<h2 id="the-second-paper-meta-learning-for-ner"><a class="markdownIt-Anchor" href="#the-second-paper-meta-learning-for-ner"></a> The second paper, Meta-Learning for NER</h2>
<p>_《Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with</p>
<p>Minimal Resources》_</p>
<h3 id="meta-learning"><a class="markdownIt-Anchor" href="#meta-learning"></a> Meta-Learning</h3>
<p>从下图中可以看出Meta-Learning算法主要分三类：学习如何设计网络的结构，学习如何优化更新参数，学习如何初始化参数。这篇论文用的就是第三类，以求能够提供更好的初始参数。</p>
<p><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/img/202004/0409-2.png" alt="Meta-learning" /></p>
<p>推荐课程：李宏毅 <a href="https://www.bilibili.com/video/BV1w4411872t/?spm_id_from=333.788.videocard.3" target="_blank" rel="noopener">Meta-Learning</a>  (<em>来自B站</em>)</p>
<h3 id="meta-training-过程"><a class="markdownIt-Anchor" href="#meta-training-过程"></a> Meta-training 过程</h3>
<p>用Meta-learning做NER任务的这篇论文的模型结构，就是在上一篇论文的多语言BERT模型基础上直接添加一个softmax层得到标注结果，重点在于它的Meta-Training的过程，整个的训练过程见伪码。</p>
<p><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/img/202004/0409-3.jpg" alt="伪码" /></p>
<ol>
<li>首先是从源语言的训练集中构建多个Pseudo-Meta-NER（伪元NER）任务。假设源语言的训练集有N个example，将每一个example都看作是一个任务的测试集，然后从源语言训练集中筛选相似度最高的K个example作为该任务的训练集。由此得到每一个任务的表示如这个式子。那为了能够计算句子的相似度，就需要首先生成句子的表示，这里的f（）函数就使用多语言Bert模型。然后根据下面这个式子计算相似度.</li>
<li>然后就有了一组任务以及初始的模型<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>M</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">M_{\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，再从任务组中随机采样一批任务T，对其中的每一个任务在sita上通过n次梯度更新得到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">\theta&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.751892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> ，这个称为内部更新。这里的Un指的就是用学习率α执行梯度下降操作n次来最小化在任务T的训练集上的损失，即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">\theta&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.751892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> 的计算公式就是公式8.</li>
<li>然后用新的参数在该任务的test集上进行评估得到该任务当前的loss值，gi是当前任务的meta梯度，β是这一过程的学习率。依次遍历所有任务将每个任务的梯度求和去更新最初的参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span> 以最小化所有任务测试集的loss和。这一步称为meta更新。</li>
</ol>
<h4 id="两个策略"><a class="markdownIt-Anchor" href="#两个策略"></a> 两个策略</h4>
<ol>
<li>
<p>Masked 策略</p>
<p>在每一个training epoch的开始，都以给定的概率去随机Masked模型的输入，以此来鼓励模型通过上下文信息去预测而非通过实体的样子。</p>
</li>
<li>
<p>Max Loss 策略</p>
<p>作者觉得对于loss来讲每一个token贡献的权重均等会使那些高loss的token的 学习不足够，所以通过该策略使模型在这些token上有更多的关注。修改后的Loss函数就是公式13.</p>
</li>
</ol>
<h3 id="adaptation-阶段即微调使用目标语言的test集"><a class="markdownIt-Anchor" href="#adaptation-阶段即微调使用目标语言的test集"></a> Adaptation 阶段，即微调，使用目标语言的test集</h3>
<p>将目标语言test集中的每一个example都看作是一个任务的test集，同样按照相似度从源语言的train集中筛选相似度最高的k个example作为任务的train集。然后在meta-training阶段得到的参数 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>θ</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\theta^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span> 上做一次梯度更新得到当前任务的新参数，以该参数在当前任务的测试集上进行测试。依次遍历完所有任务。</p>
<p>所以总的来讲，它使用的想法也几乎和多语言Bert那篇论文一样，在源语言上微调，在目标语言上测试，只是使用了Meta-learning去初始化参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span></span></span></span>。</p>
<h3 id="实验-2"><a class="markdownIt-Anchor" href="#实验-2"></a> 实验</h3>
<p><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/img/202004/0409-4.jpg" alt="Experiments" /></p>
<ol>
<li>消融实验，结果见表4，证明了每个策略都很有用。</li>
<li>个例分析，见表5。拿第一个例子来说，Base model没有识别这个实体，作者猜测可能是因为它与右上角这个很相似，而它在英语中通常不是实体，但作者的方法却可以通过学习语义信息识别到它。另外的例子也大概都是这个意思。</li>
<li><strong>结论：</strong> Meta-Learning 以及两个策略都很有效</li>
</ol>
<h2 id="reference"><a class="markdownIt-Anchor" href="#reference"></a> Reference</h2>
<p>[1] How multilingual is Multilingual BERT?</p>
<p>[2] Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with</p>
<p>Minimal Resources.</p>
<p>[3] 多语言BERT的语言表征探索. <a href="https://zhuanlan.zhihu.com/p/74524017" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/74524017</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Ian</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://ian-peace.github.io/2020/04/09/20200409paper/">http://ian-peace.github.io/2020/04/09/20200409paper/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/paper/">paper</a><a class="post-meta__tags" href="/tags/Named-Entity-Recognition/">Named Entity Recognition</a><a class="post-meta__tags" href="/tags/multilingual/">multilingual</a><a class="post-meta__tags" href="/tags/Bert/">Bert</a><a class="post-meta__tags" href="/tags/Meta-learning/">Meta-learning</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/ian-peace.png"><div class="post-qr-code__desc">微信订阅号</div></div></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/04/18/20200416paper/"><i class="fa fa-chevron-left">  </i><span>第六周paper总结</span></a></div><div class="next-post pull-right"><a href="/2020/04/02/20200401paper/"><span>第四周paper总结</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '73becf39601f4aaa0a91',
  clientSecret: '6da05639806d66834500b149e950cd54c3d3737c',
  repo: 'Blog-Comment-Repo',
  owner: 'Ian-peace',
  admin: 'Ian-peace',
  id: md5(decodeURI(location.pathname)),
  language: 'en'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/top-img-75.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2020 By Ian</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><script src="/js/search/algolia.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>