<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Adversarial Learning for NER"><meta name="keywords" content="NER,Adversarial Learning,Named entity recognition,paper"><meta name="author" content="Ian"><meta name="copyright" content="Ian"><title>Adversarial Learning for NER | 小僧有点二</title><link rel="shortcut icon" href="https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?23e67fb29291e5dc0f0f7c9f882ac0ea";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"8XJAI86SDA","apiKey":"f1fda4bfbc8839aa09272440cf9215d1","indexName":"ian-peace","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="false"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#adversarial-transfer-learning-for-chinese-named-entity-recognition-with-self-attention-mechanismmulti-task-learning"><span class="toc-number">1.</span> <span class="toc-text"> 《Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism》，Multi-task Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#问题"><span class="toc-number">1.1.</span> <span class="toc-text"> 问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#解决方法"><span class="toc-number">1.2.</span> <span class="toc-text"> 解决方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型结构"><span class="toc-number">1.3.</span> <span class="toc-text"> 模型结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练过程"><span class="toc-number">1.4.</span> <span class="toc-text"> 训练过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实验结果"><span class="toc-number">1.5.</span> <span class="toc-text"> 实验结果：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dual-adversarial-neural-transfer-for-low-resource-named-entity-recognitioncross-domain-ner"><span class="toc-number">2.</span> <span class="toc-text"> 《Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition》，Cross-domain NER</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#问题-2"><span class="toc-number">2.1.</span> <span class="toc-text"> 问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#对抗训练"><span class="toc-number">2.2.</span> <span class="toc-text"> 对抗训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实验结果-2"><span class="toc-number">2.3.</span> <span class="toc-text"> 实验结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分析"><span class="toc-number">2.4.</span> <span class="toc-text"> 分析</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/xiaoseng.png"></div><div class="author-info__name text-center">Ian</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">3</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">6</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">2</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/top-img-75.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">小僧有点二</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">Adversarial Learning for NER</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-03-17</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Machine-Learning/">Machine Learning</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.8k</span><span class="post-meta__separator">|</span><span>Reading time: 5 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>本周看的两篇论文是关于对抗学习在命名实体识别迁移学习任务上的应用，分别是18年《Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism》，提出了用于NER的多任务的对抗迁移学习方法；以及19年《Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition》，将对抗训练用于NER的跨领域迁移任务。</p>
<p>博客地址：<a href="https://ian-peace.github.io">https://ian-peace.github.io</a></p>
<a id="more"></a>
<h2 id="adversarial-transfer-learning-for-chinese-named-entity-recognition-with-self-attention-mechanismmulti-task-learning"><a class="markdownIt-Anchor" href="#adversarial-transfer-learning-for-chinese-named-entity-recognition-with-self-attention-mechanismmulti-task-learning"></a> 《Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism》，Multi-task Learning</h2>
<h3 id="问题"><a class="markdownIt-Anchor" href="#问题"></a> 问题</h3>
<ol>
<li>
<p>中文NER标注数据量很少，但中文分词任务的标注数据量很多。很直觉地想法就是利用中文分词任务的词边界信息去提升中文NER任务的表现，之前就有这样的研究，但依旧存在问题。例如：</p>
<p><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/img/202003/0317-1.png" alt="example" /></p>
<p>对于“希尔顿”、“离开”，两个任务有相同的边界，但对于“休斯顿机场”，实体识别任务的边界更加地粗粒度。而之前的工作只考虑了两个任务可以共享的信息，忽略了任务各自特有的信息，使得分词任务为实体识别任务带来了噪声。如何解决？</p>
</li>
<li>
<p>我们都知道实体识别需要使用上下文信息乃至全局信息，作者认为句子中任意字符都可以提供信息，而包括BiLSTM在内的很多模型，都无法建立句子任意两个字符之间的联系。如何捕捉全局的依赖关系？</p>
</li>
</ol>
<h3 id="解决方法"><a class="markdownIt-Anchor" href="#解决方法"></a> 解决方法</h3>
<ol>
<li>
<p>针对噪声，使用对抗学习，即提出了对抗迁移学习结构，来过滤分词任务独有的信息。</p>
</li>
<li>
<p>针对长距离依赖，使用self-attention。</p>
</li>
</ol>
<h3 id="模型结构"><a class="markdownIt-Anchor" href="#模型结构"></a> 模型结构</h3>
<ul>
<li>
<p>Embedding使用Word2vec在百度百科和微博文本语料上预训练得到。</p>
</li>
<li>
<p>特征提取部分使用BiLSTM，分为任务私有BiLSTM和共享的BiLSTM。私有的部分与各任务原有的部分一致，共享部分的公式表达见公式(4)，k代表当前的任务。公式(5)是私有部分的公式表示。</p>
</li>
<li>
<p>之后将它们的输出放入多头Self-attention中，这个没什么可说的，输出就是H‘，见公式(7)(8)。</p>
</li>
<li>
<p>然后在各自任务部分，将self-attention层的输出放入两个任务的CRF层中。要注意的是，CRF层的输入是两个部分的拼接，见公式(9)。</p>
</li>
<li>
<p><strong>对抗学习体现在中间这部分的任务判别器上</strong>，它用来判断当前句子属于哪个任务。公式(17)是这部分的损失函数，max是尽可能地使任务判别器判断正确，而min是通过梯度反转层来实现尽可能地使判别器无法区分两个任务。这样做来保证任务本身的特性不会参与到共享网络中，以至于影响其他任务的性能。</p>
<p><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/img/202003/0317-2.png" alt="model" /></p>
</li>
</ul>
<h3 id="训练过程"><a class="markdownIt-Anchor" href="#训练过程"></a> 训练过程</h3>
<p>总的Loss函数见公式(18)，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mrow><mi>N</mi><mi>E</mi><mi>R</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{NER}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.10903em;">N</span><span class="mord mathdefault mtight" style="margin-right:0.05764em;">E</span><span class="mord mathdefault mtight" style="margin-right:0.00773em;">R</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mrow><mi>C</mi><mi>W</mi><mi>S</mi></mrow></msub></mrow><annotation encoding="application/x-tex">L_{CWS}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是两个任务各自的loss。在每次迭代中，轮流选择任务，然后按批采样所选任务的数据去更新模型参数。由于两个任务收敛速度不一致，所以按实体识别任务的性能来选择是否完成训练。</p>
<h3 id="实验结果"><a class="markdownIt-Anchor" href="#实验结果"></a> 实验结果：</h3>
<ul>
<li>
<p>在weiboNER和SighanNER两个数据集上都有明显提升。</p>
</li>
<li>
<p>这篇论文并没有考虑跨领域的因素，仅仅是任务上信息的迁移。</p>
</li>
</ul>
<h2 id="dual-adversarial-neural-transfer-for-low-resource-named-entity-recognitioncross-domain-ner"><a class="markdownIt-Anchor" href="#dual-adversarial-neural-transfer-for-low-resource-named-entity-recognitioncross-domain-ner"></a> 《Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition》，Cross-domain NER</h2>
<h3 id="问题-2"><a class="markdownIt-Anchor" href="#问题-2"></a> 问题</h3>
<ol>
<li>
<p>跨领域Representation之间的差异</p>
<p>在判别器部分和上一篇论文的意思相差不多，只是将两个任务换成了两个领域，判别器用来判断当前句子属于哪个领域，被称作<strong>资源对抗判别器</strong>。</p>
<ul>
<li>两个领域各自的目的是希望判别器能够判断正确，而判别器部分也是增加了剃度反转层来尽可能地使BiLSTM部分的输出更具有资源不可知性，简单说就是让判别器无法区分两个领域的数据。</li>
<li>这么做的目的是在于，作者认为，当前NER任务的迁移学习方法都没有考虑跨领域的表示层的差异，只是强制实现了跨领域的特征表示，作者想通过这种方式，增加所提取信息的通用性和鲁棒性。具体地，作者提出了两种模型结构。</li>
</ul>
<p><img src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/img/202003/0317-3.png" alt="model" /></p>
<ul>
<li>
<p>一种是将BiLSTM单元分解为共享组件和与资源相关的组件，类似于上一篇论文，如图b所示，不过只是在判别器部分使用了self-attention；</p>
</li>
<li>
<p>另一种是，所有的BiLSTM单元都由两个资源共享，不同资源的word embedding是完全不同的，如图c所示。</p>
</li>
<li>
<p>代码实现，就是使用一个还是三个LSTM层的区别</p>
</li>
</ul>
</li>
<li>
<p>领域资源数据量不平衡</p>
<p>另外，作者还认为低资源领域NER任务还存在资源数据不平衡的问题，由于低领域数据量小，梯度下降会让模型更加偏向高领域。为此，作者在两个领域资源上添加了权重<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>来平衡它们对模型的影响。</p>
<p>但就算添加了权重，实验显示，高资源中的易分类样本依旧构成了大部分损失并主导了梯度。所以作者在资源对抗判别器基础上提出了广义资源对抗判别器（GRAD），公式(1)是GRAD的损失函数，它添加了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span></span></span></span>值来表示来自hard samples和easy samples的损失贡献的比值。公式(2)是整个模型的损失函数。</p>
</li>
</ol>
<h3 id="对抗训练"><a class="markdownIt-Anchor" href="#对抗训练"></a> 对抗训练</h3>
<ul>
<li>
<p>在原始样本的char 和 word embedding上增加扰动，生成对抗样本的embedding，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>a</mi><mi>d</mi><mi>v</mi></mrow></msub><mo>=</mo><mi>x</mi><mo>+</mo><msub><mi>η</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">x_{adv}=x+\eta_x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>；</p>
<p>（计算扰动的方式，同15年第一次提出对抗训练的论文《Explaining and Harnessing Adversarial Examples》，可以自行去看）</p>
</li>
<li>
<p>对分类器进行原始样本和对抗样本的混合训练，提高模型的泛化能力；</p>
</li>
<li>
<p>例如：对于含有word embedding <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mi>S</mi></msub></mrow><annotation encoding="application/x-tex">W_S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的源数据，对抗训练的损失可以定义为：</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>l</mi><mrow><mi>A</mi><mi>T</mi></mrow></msub><mo>=</mo><mi>l</mi><mo stretchy="false">(</mo><mi mathvariant="normal">Θ</mi><mo separator="true">;</mo><msub><mi>W</mi><mi>S</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>l</mi><mo stretchy="false">(</mo><mi mathvariant="normal">Θ</mi><mo separator="true">;</mo><msub><mi>W</mi><mrow><mi>S</mi><mo separator="true">,</mo><mi>a</mi><mi>d</mi><mi>v</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">l_{AT}=l(\Theta;W_S)+l(\Theta;W_{S,adv})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">A</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord">Θ</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord">Θ</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>S</mi><mo separator="true">,</mo><mi>a</mi><mi>d</mi><mi>v</mi></mrow></msub><mo>=</mo><msub><mi>W</mi><mi>S</mi></msub><mo>+</mo><msub><mi>η</mi><msub><mi>W</mi><mi>S</mi></msub></msub></mrow><annotation encoding="application/x-tex">W_{S,adv}=W_S+\eta_{W_S}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.680865em;vertical-align:-0.250305em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">η</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567071428571427em;margin-left:-0.13889em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.14329285714285717em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.250305em;"><span></span></span></span></span></span></span></span></span></span></p>
</li>
</ul>
<h3 id="实验结果-2"><a class="markdownIt-Anchor" href="#实验结果-2"></a> 实验结果</h3>
<p>这篇论文其实做了跨领域实体识别和跨语言实体识别两个任务，分别通过提出的两个模型进行了实验。实验得出，对于极低资源的跨语言迁移和跨领域迁移，DATNet-F可以获得更好的性能；对于具有相对较多训练数据的跨语言迁移，则应选择DATNet-P模型。</p>
<h3 id="分析"><a class="markdownIt-Anchor" href="#分析"></a> 分析</h3>
<p>论文分析DATNet-F模型在跨领域迁移任务上性能好的原因是，由于跨领域迁移使用相同的语言，两个领域很多信息是共有的，所以需要更多的共享隐藏特性来承载这些知识。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Ian</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://ian-peace.github.io/2020/03/17/Adversarial%20Learning%20for%20NER/">http://ian-peace.github.io/2020/03/17/Adversarial%20Learning%20for%20NER/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NER/">NER</a><a class="post-meta__tags" href="/tags/Adversarial-Learning/">Adversarial Learning</a><a class="post-meta__tags" href="/tags/Named-entity-recognition/">Named entity recognition</a><a class="post-meta__tags" href="/tags/paper/">paper</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/ian-peace.png"><div class="post-qr-code__desc">微信订阅号</div></div></div><nav id="pagination"><div class="next-post pull-right"><a href="/2020/03/14/Hexo+GitHub%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/"><span>Hexo+GitHub搭建博客</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '73becf39601f4aaa0a91',
  clientSecret: '6da05639806d66834500b149e950cd54c3d3737c',
  repo: 'Blog-Comment-Repo',
  owner: 'Ian-peace',
  admin: 'Ian-peace',
  id: md5(decodeURI(location.pathname)),
  language: 'en'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(https://ian-1259157142.cos.ap-beijing.myqcloud.com/Blog_self/top-img-75.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2020 By Ian</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><script src="/js/search/algolia.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>